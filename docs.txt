## problem statement
# USA visa approval status
- given certain set of features such as continent, education, job-experience, training, employment, current age, etc.
- we have to predict the probability of a visa approval.



# tools and tech
- Python, MongoDB, Flask, Scikit-learn, Pandas, NumPy, Matplotlib, Seaborn



# Solution scope
- This can be used on real life by US visa applicants so that they can improve their resume and criteria to get visa approved.



# Solution Approach
- Machine Learning (classification algorithms)



# Solution Proposed
- Load the data from MongoDB
- Perform EDA and feature engineering to select the desirable features
- Fit the ML classification algorithm and find out the best model
- Select top few and tune the hyperparameters
- Select the best model based on desired metrics



# Project Setup
- GitHub Repository
- Requirements
- Template



# Resources:
- MLOps tool: https://www.evidentlyai.com/
- MongoDB: https://www.mongodb.com/


Step 2
# Setup Database
# Logging Module
# Exception Module
# Utility Module

# From keggle to MongoDB connect it
    - Do data ingestion on DB

# Create the MDB Account
Dataset: https://www.kaggle.com/datasets/moro23/easyvisa-dataset

Created custom exception, custom logger, custom utility functions

Next step:
I am doing EDA, Feature engineering, model training on Jupyter Notebook

Next Step:
We implement data ingestion
 - Data Drift with evidentlyai (mlops tool)
 - Understanding Data Validation, Data Transformation, Model Training, Model Evaluation, Model Pusher
 - Model Training
 - Model Evaluation
 - Model Pusher

Creating a Pipeline
  > Component 1 Data ingestion -> connect to my mongodb using connection string -> <- fetch the data

## Workflow of Pipeline
  1. constants create ( any changes must be made in this folder to reflect in everything automatically)
  2. entity create
  3. components create
  4. pipelin update

    - how artifacts created:
        constants -> cinfig entity -> Data Ingestion -> (Train.parquet, Test.parquet) Artifacts
    - after ingestuon:
        (Train.parquet, Test.parquet) Artifacts -> Data Validation -> Data Transformation -> Model Trainer -> Model Evaluation -> Model Pusher

## Develop all the components
- Data Ingestion developed succesfull for training Pipeline

## Data Drift with evidentlyai amd data Validation
We will solve this by data Transformation then Evaluation

## Now I am doing Data Transformation & Model Training
in this method it will return preprocessing_pkl, train, test data using these data we will get trained_model

## Now developing model trainer components
I am using neuro-mf to automate the task of model selection and hyperparameters tuning for model trainig module in components, 
using trainig pipeline module to run everything in one place.

## Now I am doing Model Evaluation, Pusher & Prediction
- Model Evaluation
- Model Pusher to production (Amazon S3)
- Create the Prediction Pipeline
- User End app using FastAPI

workflow: ModelTrainer gave me model.pkl -> do model evaluation -> 
I am making cloudflare r2 bucket to store and retrieve my model
Created estimator entity, created r2 storage connection object, created r2 configuration
now the code is ready now I will write config entity and artifact entity, later I will develop components for pipeline
